{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import cv2\n","import re\n","import gc\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from matplotlib.image import imread\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","import tensorflow.keras as K\n","from keras import applications"],"metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Config and loading data"],"metadata":{}},{"cell_type":"code","source":["# Configuration\n","\n","# For Kaggle submission, set to True\n","SUBMIT = False\n","\n","# Use only 1000 images when True\n","DEBUG = False\n","\n","# Which part of the model is used\n","IMG = True\n","EXTRA_LAYERS = False\n","TEXT = True\n","PHASH = False\n","SIM_PHASH = False\n","\n","\n","PATH = '../input/shopee-product-matching/'\n","IMG_SIZE = 456 # avg img size = 444, EfficientNetB5 = 456\n","BATCH = 5000\n","IMG_SIM_THRESHOLD = 0.15\n","TEXT_SIM_THRESHOLD = 0.45\n","PHASH_SIM_THRESHOLD = 6"],"metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Load data\n","test = pd.read_csv(PATH + 'test.csv')\n","train = pd.read_csv(PATH + 'train.csv')"],"metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if DEBUG:\n","    train = train.sample(n = 1000).reset_index(drop = True)\n","\n","if SUBMIT: \n","    img_path = PATH +\"/test_images/\"\n","    train = test \n","else:\n","    img_path = PATH +\"/train_images/\"\n","    # convert label_group to target\n","    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n","    train['target'] = train.label_group.map(tmp)\n","    train"],"metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Image predictions"],"metadata":{}},{"cell_type":"code","source":["# images pre-processing\n","\n","class DataGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, df, img_size=IMG_SIZE, batch_size=32, path=''): \n","        self.df = df\n","        self.img_size = img_size\n","        self.batch_size = batch_size\n","        self.path = path\n","        self.indexes = np.arange( len(self.df) )\n","        \n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        ct = len(self.df) // self.batch_size\n","        ct += int(( (len(self.df)) % self.batch_size)!=0)\n","        return ct\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","        X = self.__data_generation(indexes)\n","        return X\n","            \n","    def __data_generation(self, indexes):\n","        'Generates data containing batch_size samples' \n","        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n","        df = self.df.iloc[indexes]\n","        for i,(index,row) in enumerate(df.iterrows()):\n","            img = cv2.imread(self.path + row['image'])\n","            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n","        return X"],"metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### image embedding with EfficentNet"],"metadata":{}},{"cell_type":"code","source":["# load pre-trained model\n","# \"tf keras efficientnet imagenet no top\" must be added with \"+ Add data\"\n","if IMG:\n","    \n","    WGT = \"../input/tfkerasefficientnetimagenetnotop/efficientnetb5_notop.h5\"\n","    eff_net = K.applications.EfficientNetB5(weights=WGT, input_shape=None, include_top=False, pooling=\"avg\", drop_connect_rate=0.2)\n","    #WGT = \"../input/tfkerasefficientnetimagenetnotop/efficientnetb0_notop.h5\"\n","    #eff_net = K.applications.EfficientNetB0(weights=WGT, input_shape=None, include_top=False, pooling=\"avg\", drop_connect_rate=0.2)\n","    \n","    if EXTRA_LAYERS == False: model = eff_net"],"metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# apply images to pre-trained model (EfficientNet)\n","if IMG:\n","    \n","    #chunk = BATCH \n","    #cls = len(train) // chunk \n","    #cls += int (len(train) % chunk != 0)\n","    #image_embedding = []\n","    #for i in tqdm(range(cls)) :\n","\n","    #    a = i * chunk \n","    #    b = (i+1) * chunk \n","    #    b = min(b,len(train))\n","    #    data = DataGenerator(train.iloc[a:b], path=img_path)\n","    #    emb = model.predict(data, use_multiprocessing=True, workers=8)\n","    #    image_embedding.append(emb)\n","\n","    data = DataGenerator(train, path=img_path)\n","    image_embedding = model.predict(data, use_multiprocessing=True, workers=8)\n","    \n","    del(model)\n","    gc.collect()"],"metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### NearestNeighbors on image embeddings"],"metadata":{}},{"cell_type":"code","source":["#IMG_SIM_THRESHOLD = 0.15\n","# get images prediction from nearest image embeddings\n","if IMG:\n","    \n","    nn = 50\n","    # prevent bug while submiting to kaggle\n","    if len(test)==3: nn = 3\n","    knn = NearestNeighbors(n_neighbors=3, metric =\"cosine\")\n","    knn.fit(image_embedding)\n","    \n","    chunk = BATCH\n","    cl = len(train) // chunk \n","    cl += int((len(train) % chunk) !=0)\n","    pred_img = []\n","    for i in tqdm(range(cl)) :\n","\n","        a = i * chunk\n","        b = (i+1) * chunk\n","        b = min(len(train),b)\n","        distances,indices = knn.kneighbors(image_embedding[a:b,])\n","        for j in range(b-a):\n","            distance = distances[j,:]\n","            ind = np.where(distance < IMG_SIM_THRESHOLD)[0]\n","            IND = indices[j,ind]\n","            pred_img.append(train.iloc[IND].posting_id.values)\n","\n","    train[\"pred_img\"] = pred_img"],"metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 7/7 [01:45<00:00, 15.03s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## Text predictions"],"metadata":{}},{"cell_type":"code","source":["# text pre-processing\n","\n","def letters_only(t):\n","    return re.sub(\"[^a-zA-Z]\", \" \", t)\n","\n","def lowercase_only(t):\n","    return t.lower()\n","\n","def remove_patterns(t):\n","    '''\n","    Some titles contain \" x \" as separators.\n","    Example: train_3369186413\n","    '''\n","    patterns = {' x ': ' ',\n","               ' a ': ' '}\n","    for k, v in patterns.items():\n","        t = t.replace(k,v)\n","    return t\n","\n","def replace_multispace_by_space(t):\n","    return re.sub('\\s+',' ',t)\n","\n","#cm_map adjusted - Source of original cm_map : https://www.kaggle.com/c/shopee-product-matching/discussion/228358\n","cm_map = {\"wanita\": \"woman\", \n","          \"anak\": \"child\", \n","          \"bayi\": \"baby\",\n","          \"tas\": \"bag\", \n","          \"masker\": \"face mask\", \n","          \"pria\": \"men\",\n","          \"murah\": \"cheap\",\n","          \"tangan\": \"hand\", \n","          \"alat\": \"tool\", \n","          \"motif\": \"motive\", \n","          \"warna\": \"color\", \n","          \"bahan\": \"material\", \n","          \"celana\": \"pants\", \n","          \"baju\": \"clothes\", \n","          \"kaos\": \"t-shirt\", \n","          \"sepatu\": \"shoes\", \n","          \"rambut\": \"hair\", \n","          \"mainan\": \"toy\", \n","          \"sarung\": \"holster\", \n","          \"polos\": \"plain\", \n","          \"rak\": \"rack\", \n","          \"botol\": \"bottle\", \n","          \"sabun\": \"soap\", \n","          \"kain\": \"fabric\", \n","          \"panjang\": \"long\", \n","          \"kabel\": \"cable\", \n","          \"buku\": \"book\", \n","          \"plastik\": \"plastic\", \n","          \"mobil\": \"car\", \n","          \"hitam\": \"black\", \n","          \"karakter\": \"character\", \n","          \"putih\": \"white\", \n","          \"dompet\": \"purse\", \n","          \"kaki\": \"feet\", \n","          \"pembersih\": \"cleaners\", \n","          \"lipat\": \"folding\", \n","          \"silikon\": \"silicone\", \n","          \"minyak\": \"oil\", \n","          \"isi\": \"contents\", \n","          \"paket\": \"package\", \n","          \"susu\": \"milk\", \n","          \"gamis\": \"robe\", \n","          \"mandi\": \"bath\", \n","          \"madu\": \"honey\", \n","          \"kulit\": \"skin\", \n","          \"serbaguna\": \"multipurpose\", \n","          \"bisa\": \"can\", \n","          \"kacamata\": \"spectacles\", \n","          \"pendek\": \"short\", \n","          \"tali\": \"rope\", \n","          \"selempang\": \"sash\",\n","          \"topi\": \"hat\", \n","          \"obat\": \"drug\", \n","          \"gantungan\": \"hanger\", \n","          \"tahun\": \"year\", \n","          \"jilbab\": \"hijab\", \n","          \"dapur\": \"kitchen\", \n","          \"dinding\": \"wall\",\n","          \"kuas\": \"brush\",\n","          \"perempuan\": \"woman\",\n","          \"katun\": \"cotton\", \n","          \"sepeda\": \"bike\", \n","          \"lucu\": \"funny\", \n","          \"lengan\": \"arm\", \n","          \"kaca\": \"glass\", \n","          \"garansi\": \"warranty\", \n","          \"bunga\": \"flower\", \n","          \"handuk\": \"towel\", \n","          \"dewasa\": \"adult\", \n","          \"elektrik\": \"electric\", \n","          \"timbangan\": \"balance\", \n","          \"besar\": \"big\", \n","          \"bahan\": \"ingredient\", \n","          \"ransel\": \"backpack\", \n","          \"kertas\": \"paper\",\n","          \"lampu\" : \"light\",\n","          \"sepatu\": \"shoes\",\n","          \"tempat\": \"place\"}\n","\n","def translate_ind_to_eng(t):\n","    res = \" \".join(cm_map.get(w, w) for w in t.split())    \n","    return res"],"metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# text clean up\n","# removed .apply(letters_only)\n","if TEXT:\n","    X = (train['title'].apply(lowercase_only)\n","                       .apply(remove_patterns)\n","                       .apply(replace_multispace_by_space)\n","                       .apply(translate_ind_to_eng))"],"metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#TEXT_SIM_THRESHOLD = 0.45\n","# get texts prediction from nearest text embeddings\n","if TEXT:\n","    \n","    #vectorizer = CountVectorizer()\n","    vectorizer =  TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')\n","    X_mat = vectorizer.fit_transform(X)\n","\n","    nn = 50\n","    # prevent bug while submitting to kaggle\n","    if len(test)==3: nn = 3\n","    knn = NearestNeighbors(n_neighbors=3, metric =\"cosine\")\n","    knn.fit(X_mat)\n","\n","    # get images prediction from nearest image embeddings\n","    chunk = BATCH\n","    cl = len(train) // chunk \n","    cl += int((len(train) % chunk) !=0)\n","    pred_text = []\n","    for i in tqdm(range(cl)) :\n","\n","        a = i * chunk\n","        b = (i+1) * chunk\n","        b = min(len(train),b)\n","        distances,indices = knn.kneighbors(X_mat[a:b,])\n","        for j in range(b-a):\n","            distance = distances[j,:]\n","            ind = np.where(distance < TEXT_SIM_THRESHOLD)[0]\n","            IND = indices[j,ind]\n","            pred_text.append(train.iloc[IND].posting_id.values)\n","\n","    train[\"pred_text\"] = pred_text"],"metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 7/7 [00:38<00:00,  5.55s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## pHash predictions"],"metadata":{}},{"cell_type":"code","source":["# group identical phash\n","if PHASH:\n","    tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\n","    train['pred_phash'] = train.image_phash.map(tmp)"],"metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# group similar phash\n","if SIM_PHASH:\n","    def hamming_distance(hash1, hash2):  \n","        return sum([c1 != c2 for c1, c2 in zip(hash1, hash2)])\n","\n","    # very long to run and there's no gain compared to pred_img\n","    \n","    # check all phash with hamming distance < 6\n","    train['pred_phash'] = np.nan\n","    for i in tqdm(range(train.shape[0])):\n","        train['dist'] = train['image_phash'].apply(lambda x: hamming_distance(x,train['image_phash'].iloc[i]))\n","        train['pred_phash'].iloc[i] =  [x for x in train['posting_id'].loc[train['dist'] < PHASH_SIM_THRESHOLD]]"],"metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Scoring"],"metadata":{}},{"cell_type":"code","source":["# f1 score\n","def getMetric(col):\n","    def f1score(row):\n","        n = len( np.intersect1d(row.target,row[col]) )\n","        return 2*n / (len(row.target)+len(row[col]))\n","    return f1score"],"metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["if SUBMIT == False: \n","    if PHASH:\n","        print(\"f1 score for phash :\", round(train.apply(getMetric('pred_phash'),axis=1).mean(), 3))\n","    if TEXT:\n","        print(\"f1 score for text :\", round(train.apply(getMetric('pred_text'),axis=1).mean(), 3))\n","    if IMG:\n","        print(\"f1 score for images :\", round(train.apply(getMetric('pred_img'),axis=1).mean(), 3))"],"metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"f1 score for phash : 0.553\nf1 score for text : 0.624\nf1 score for images : 0.613\n","output_type":"stream"}]},{"cell_type":"code","source":["def combine_predictions(row):\n","    if PHASH and TEXT and IMG:\n","        y = np.concatenate([row['pred_phash'], row['pred_text'], row['pred_img']])\n","    if TEXT and IMG:    \n","        y = np.concatenate([row['pred_text'], row['pred_img']])\n","    if PHASH and TEXT:    \n","        y = np.concatenate([row['pred_phash'], row['pred_text']])\n","    if PHASH and IMG:    \n","        y = np.concatenate([row['pred_phash'], row['pred_img']])\n","    return list(np.unique(y))\n","                        \n","train['pred'] = train.apply(lambda x: combine_predictions(x),axis=1)"],"metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["if SUBMIT == False: \n","    print(\"f1 score for combined pred :\", round(train.apply(getMetric('pred'),axis=1).mean(), 3))"],"metadata":{"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"f1 score for combined pred : 0.692\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## Submission"],"metadata":{}},{"cell_type":"code","source":["# prepare results for submission\n","\n","submission = pd.DataFrame()\n","\n","def combine_pred(row):\n","    if PHASH and TEXT and IMG:\n","        y = np.concatenate([row['pred_phash'], row['pred_text'], row['pred_img']])\n","    if TEXT and IMG:\n","        y = np.concatenate([row['pred_text'], row['pred_img']]) \n","    if PHASH and TEXT:\n","        y = np.concatenate([row['pred_phash'], row['pred_text']])    \n","    if PHASH and IMG:    \n","        y = np.concatenate([row['pred_phash'], row['pred_img']])\n","    return \" \".join(np.unique(y)) \n","\n","submission['posting_id'] = train['posting_id']\n","submission['matches'] = train.apply(lambda x: combine_pred(x),axis=1)\n","\n","submission.to_csv(\"submission.csv\",index = False)\n","submission.head()"],"metadata":{"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"         posting_id                                            matches\n0   train_129225211                   train_129225211 train_2278313361\n1  train_3386243561  train_1816968361 train_2120597446 train_338624...\n2  train_2288590299                  train_2288590299 train_3803689425\n3  train_2406599165  train_1744956981 train_2406599165 train_352677...\n4  train_3369186413                                   train_3369186413","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>train_1816968361 train_2120597446 train_338624...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>train_1744956981 train_2406599165 train_352677...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>train_3369186413</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}